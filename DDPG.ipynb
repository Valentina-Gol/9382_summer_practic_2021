{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valentina-Gol/9382_summer_practic_2021/blob/main/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0tx19BPypdeI",
        "outputId": "a65f0fb5-d48b-42da-d5e9-ad92952a4931"
      },
      "source": [
        "!rm -rf gym-duckietown/\n",
        "branch = \"daffy\" #@param ['master', 'daffy']\n",
        "! git clone --branch {branch} https://github.com/duckietown/gym-duckietown\n",
        "!pip install  gym-duckietown/.\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!apt-get install x11-utils\n",
        "\n",
        "#! pip uninstall scipy\n",
        "#! pip install scipy==1.1.0\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "def create_display():\n",
        "    display = Display(visible=False, size=(1400, 900))\n",
        "    d = display.start()\n",
        "    if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "        !bash ../xvfb start\n",
        "        %env DISPLAY=:1\n",
        "    return d\n",
        "\n",
        "display = create_display()\n",
        "from gym_duckietown.envs import DuckietownEnv\n",
        "from gym_duckietown.simulator import Simulator\n",
        "import gym\n",
        "import numpy as np\n",
        "import pyglet\n",
        "\n",
        "!pip3 install git+https://github.com/OSLL/duckietown-segmentation.git@seg1\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gym-duckietown'...\n",
            "remote: Enumerating objects: 6045, done.\u001b[K\n",
            "remote: Counting objects: 100% (326/326), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 6045 (delta 158), reused 254 (delta 112), pack-reused 5719\u001b[K\n",
            "Receiving objects: 100% (6045/6045), 79.14 MiB | 31.11 MiB/s, done.\n",
            "Resolving deltas: 100% (3467/3467), done.\n",
            "Processing ./gym-duckietown\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: gym>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (1.19.5)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (1.5.0)\n",
            "Requirement already satisfied: pyzmq>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (22.3.0)\n",
            "Requirement already satisfied: scikit-image>=0.13.1 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (0.16.2)\n",
            "Requirement already satisfied: opencv-python>=3.4 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (3.13)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (1.3.0)\n",
            "Requirement already satisfied: duckietown-world-daffy in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (6.2.24)\n",
            "Requirement already satisfied: PyGeometry-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (2.0.5)\n",
            "Requirement already satisfied: carnivalmirror==0.6.2 in /usr/local/lib/python3.7/dist-packages (from duckietown-gym-daffy==6.1.16) (0.6.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.1->duckietown-gym-daffy==6.1.16) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2.2->carnivalmirror==0.6.2->duckietown-gym-daffy==6.1.16) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet->duckietown-gym-daffy==6.1.16) (0.16.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13.1->duckietown-gym-daffy==6.1.16) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13.1->duckietown-gym-daffy==6.1.16) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13.1->duckietown-gym-daffy==6.1.16) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13.1->duckietown-gym-daffy==6.1.16) (7.1.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.3.4)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.9.33)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (4.2.6)\n",
            "Requirement already satisfied: PyContracts3 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.0.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (4.4.1)\n",
            "Requirement already satisfied: zuper-commons-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (6.1.7)\n",
            "Requirement already satisfied: gltflib in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0.12)\n",
            "Requirement already satisfied: duckietown-serialization-ds1<2 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0.10)\n",
            "Requirement already satisfied: oyaml in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0)\n",
            "Requirement already satisfied: zuper-ipce-z6 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (6.0.36)\n",
            "Requirement already satisfied: pyrender in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.1.45)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4<=4.7.1,>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (4.6.3)\n",
            "Requirement already satisfied: zuper-typing-z6>=6.0.66 in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (6.1.8)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.4.4)\n",
            "Requirement already satisfied: aido-protocols-daffy in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (6.0.41)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.7/dist-packages (from duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (15.0.1)\n",
            "Requirement already satisfied: base58<2 in /usr/local/lib/python3.7/dist-packages (from duckietown-serialization-ds1<2->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0.3)\n",
            "Requirement already satisfied: validate-email in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.3)\n",
            "Requirement already satisfied: mypy-extensions in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.4.3)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (2.0.6)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.3.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (2.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.7.4.3)\n",
            "Requirement already satisfied: coverage>=1.4.33 in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.7.1)\n",
            "Requirement already satisfied: pybase64 in /usr/local/lib/python3.7/dist-packages (from zuper-typing-z6>=6.0.66->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.2.0)\n",
            "Requirement already satisfied: xtermcolor in /usr/local/lib/python3.7/dist-packages (from zuper-commons-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.3)\n",
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.7/dist-packages (from zuper-commons-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.11.1)\n",
            "Requirement already satisfied: zuper-nodes-z6>=6.0.37 in /usr/local/lib/python3.7/dist-packages (from aido-protocols-daffy->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (6.2.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.7/dist-packages (from coloredlogs->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (10.0)\n",
            "Requirement already satisfied: dataclasses-json>=0.4.5 in /usr/local/lib/python3.7/dist-packages (from gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.5.6)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from dataclasses-json>=0.4.5->gltflib->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.6.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from PyContracts3->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (4.4.2)\n",
            "Requirement already satisfied: PyOpenGL==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pyrender->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (3.1.0)\n",
            "Requirement already satisfied: freetype-py in /usr/local/lib/python3.7/dist-packages (from pyrender->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (57.4.0)\n",
            "Requirement already satisfied: cbor2<6,>=5 in /usr/local/lib/python3.7/dist-packages (from zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (5.4.2)\n",
            "Requirement already satisfied: py-multihash in /usr/local/lib/python3.7/dist-packages (from zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.2.3)\n",
            "Requirement already satisfied: py-cid in /usr/local/lib/python3.7/dist-packages (from zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.3.0)\n",
            "Requirement already satisfied: py-multibase<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0.3)\n",
            "Requirement already satisfied: py-multicodec<0.3.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (0.2.1)\n",
            "Requirement already satisfied: morphys<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0)\n",
            "Requirement already satisfied: python-baseconv<2.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from py-multibase<2.0.0,>=1.0.0->py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.2.2)\n",
            "Requirement already satisfied: varint<2.0.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from py-multicodec<0.3.0->py-cid->zuper-ipce-z6->duckietown-world-daffy->duckietown-gym-daffy==6.1.16) (1.0.2)\n",
            "Building wheels for collected packages: duckietown-gym-daffy\n",
            "  Building wheel for duckietown-gym-daffy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for duckietown-gym-daffy: filename=duckietown_gym_daffy-6.1.16-py3-none-any.whl size=130470 sha256=b14b610aca023d63c368295cf6fefbe32e2195d8827f96f6d7c07d44a3ae4862\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/3f/98/22d1fb54f535b9a5c5d17f1d026b512440cbd9a859fc894f20\n",
            "Successfully built duckietown-gym-daffy\n",
            "Installing collected packages: duckietown-gym-daffy\n",
            "  Attempting uninstall: duckietown-gym-daffy\n",
            "    Found existing installation: duckietown-gym-daffy 6.1.16\n",
            "    Uninstalling duckietown-gym-daffy-6.1.16:\n",
            "      Successfully uninstalled duckietown-gym-daffy-6.1.16\n",
            "Successfully installed duckietown-gym-daffy-6.1.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym_duckietown"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.7/dist-packages (from piglet) (1.2.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.4.7)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Collecting git+https://github.com/OSLL/duckietown-segmentation.git@seg1\n",
            "  Cloning https://github.com/OSLL/duckietown-segmentation.git (to revision seg1) to /tmp/pip-req-build-mjf6r642\n",
            "  Running command git clone -q https://github.com/OSLL/duckietown-segmentation.git /tmp/pip-req-build-mjf6r642\n",
            "  Running command git checkout -b seg1 --track origin/seg1\n",
            "  Switched to a new branch 'seg1'\n",
            "  Branch 'seg1' set up to track remote branch 'seg1' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from segmentation==0.1.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from segmentation==0.1.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from segmentation==0.1.0) (0.10.0+cu111)\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from segmentation==0.1.0) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6->segmentation==0.1.0) (1.1.0)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6->segmentation==0.1.0) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6->segmentation==0.1.0) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (7.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (0.16.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (1.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (1.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6->segmentation==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->segmentation==0.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->segmentation==0.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->segmentation==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->segmentation==0.1.0) (2021.5.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->segmentation==0.1.0) (3.7.4.3)\n",
            "Building wheels for collected packages: segmentation, albumentations\n",
            "  Building wheel for segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation: filename=segmentation-0.1.0-py3-none-any.whl size=8967 sha256=3a02e0bb45f88bb6416b86d5c8e1a47c4d7f34820c6eded389fe495678e63092\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b06pkmdp/wheels/a8/d2/1a/63b4f2ac163df9e54ac3893363821c3bdeb17f8df6bef49fa6\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65172 sha256=043ee57576a45a2148960fce81f03a92ec7871de157f853675de6a94b6152105\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built segmentation albumentations\n",
            "Installing collected packages: imgaug, albumentations, segmentation\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0 segmentation-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnr5LDrFma4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgwtedKhISLp"
      },
      "source": [
        "import functools\n",
        "import operator\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Implementation of Deep Deterministic Policy Gradients (DDPG)\n",
        "# Paper: https://arxiv.org/abs/1509.02971\n",
        "\n",
        "\n",
        "class ActorDense(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(ActorDense, self).__init__()\n",
        "\n",
        "        state_dim = functools.reduce(operator.mul, state_dim, 1)\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_action * self.tanh(self.l3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ActorCNN(nn.Module):\n",
        "    def __init__(self, action_dim, max_action):\n",
        "        super(ActorCNN, self).__init__()\n",
        "\n",
        "        # ONLY TRU IN CASE OF DUCKIETOWN:\n",
        "        flat_size = 32 * 9 * 14\n",
        "\n",
        "        self.lr = nn.LeakyReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 8, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 4, stride=2)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 4, stride=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lin1 = nn.Linear(flat_size, 512)\n",
        "        self.lin2 = nn.Linear(512, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.lr(self.conv1(x)))\n",
        "        x = self.bn2(self.lr(self.conv2(x)))\n",
        "        x = self.bn3(self.lr(self.conv3(x)))\n",
        "        x = self.bn4(self.lr(self.conv4(x)))\n",
        "        x = x.reshape(x.size(0), -1)  # flatten\n",
        "        x = self.dropout(x)\n",
        "        x = self.lr(self.lin1(x))\n",
        "\n",
        "        # this is the vanilla implementation\n",
        "        # but we're using a slightly different one\n",
        "        # x = self.max_action * self.tanh(self.lin2(x))\n",
        "\n",
        "        # because we don't want our duckie to go backwards\n",
        "        x = self.lin2(x)\n",
        "        x[:, 0] = self.max_action * self.sigm(x[:, 0])  # because we don't want the duckie to go backwards\n",
        "        x[:, 1] = self.tanh(x[:, 1])\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CriticDense(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(CriticDense, self).__init__()\n",
        "\n",
        "        state_dim = functools.reduce(operator.mul, state_dim, 1)\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400 + action_dim, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(torch.cat([x, u], 1)))\n",
        "        x = self.l3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CriticCNN(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super(CriticCNN, self).__init__()\n",
        "\n",
        "        flat_size = 32 * 9 * 14\n",
        "\n",
        "        self.lr = nn.LeakyReLU()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 8, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(32, 32, 4, stride=2)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 4, stride=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lin1 = nn.Linear(flat_size, 256)\n",
        "        self.lin2 = nn.Linear(256 + action_dim, 128)\n",
        "        self.lin3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, states, actions):\n",
        "        x = self.bn1(self.lr(self.conv1(states)))\n",
        "        x = self.bn2(self.lr(self.conv2(x)))\n",
        "        x = self.bn3(self.lr(self.conv3(x)))\n",
        "        x = self.bn4(self.lr(self.conv4(x)))\n",
        "        x = x.reshape(x.size(0), -1)  # flatten\n",
        "        x = self.lr(self.lin1(x))\n",
        "        x = self.lr(self.lin2(torch.cat([x, actions], 1)))  # c\n",
        "        x = self.lin3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DDPG(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action, net_type):\n",
        "        super(DDPG, self).__init__()\n",
        "        print(\"Starting DDPG init\")\n",
        "        assert net_type in [\"cnn\", \"dense\"]\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "        if net_type == \"dense\":\n",
        "            self.flat = True\n",
        "            self.actor = ActorDense(state_dim, action_dim, max_action).to(device)\n",
        "            self.actor_target = ActorDense(state_dim, action_dim, max_action).to(device)\n",
        "        else:\n",
        "            self.flat = False\n",
        "            self.actor = ActorCNN(action_dim, max_action).to(device)\n",
        "            self.actor_target = ActorCNN(action_dim, max_action).to(device)\n",
        "\n",
        "        print(\"Initialized Actor\")\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        print(\"Initialized Target+Opt [Actor]\")\n",
        "        if net_type == \"dense\":\n",
        "            self.critic = CriticDense(state_dim, action_dim).to(device)\n",
        "            self.critic_target = CriticDense(state_dim, action_dim).to(device)\n",
        "        else:\n",
        "            self.critic = CriticCNN(action_dim).to(device)\n",
        "            self.critic_target = CriticCNN(action_dim).to(device)\n",
        "        print(\"Initialized Critic\")\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "        print(\"Initialized Target+Opt [Critic]\")\n",
        "\n",
        "    def predict(self, state):\n",
        "\n",
        "        # just making sure the state has the correct format, otherwise the prediction doesn't work\n",
        "        assert state.shape[0] == 3\n",
        "\n",
        "        if self.flat:\n",
        "            state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        else:\n",
        "            state = torch.FloatTensor(np.expand_dims(state, axis=0)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=64, discount=0.99, tau=0.001):\n",
        "        print('Training!!!')\n",
        "\n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Sample replay buffer\n",
        "            sample = replay_buffer.sample(batch_size, flat=self.flat)\n",
        "            state = torch.FloatTensor(sample[\"state\"]).to(device)\n",
        "            action = torch.FloatTensor(sample[\"action\"]).to(device)\n",
        "            next_state = torch.FloatTensor(sample[\"next_state\"]).to(device)\n",
        "            done = torch.FloatTensor(1 - sample[\"done\"]).to(device)\n",
        "            reward = torch.FloatTensor(sample[\"reward\"]).to(device)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
        "            target_Q = reward + (done * discount * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimate\n",
        "            current_Q = self.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "            # Optimize the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Compute actor loss\n",
        "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "            # Optimize the actor\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # Update the frozen target models\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        print(\"Saving to {}/{}_[actor|critic].pth\".format(directory, filename))\n",
        "        torch.save(self.actor.state_dict(), \"{}/{}_actor.pth\".format(directory, filename))\n",
        "        print(\"Saved Actor\")\n",
        "        torch.save(self.critic.state_dict(), \"{}/{}_critic.pth\".format(directory, filename))\n",
        "        print(\"Saved Critic\")\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.actor.load_state_dict(\n",
        "            torch.load(\"{}/{}_actor.pth\".format(directory, filename), map_location=device)\n",
        "        )\n",
        "        self.critic.load_state_dict(\n",
        "            torch.load(\"{}/{}_critic.pth\".format(directory, filename), map_location=device)\n",
        "        )\n",
        "\n",
        "\n",
        "def seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "# Code based on:\n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "\n",
        "# Simple replay buffer\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "\n",
        "    # Expects tuples of (state, next_state, action, reward)\n",
        "    def add(self, state, next_state, action, reward):\n",
        "        if len(self.storage) < self.max_size:\n",
        "            self.storage.append((state, next_state, action, reward))\n",
        "        else:\n",
        "            # Remove random element in the memory beforea adding a new one\n",
        "            self.storage.pop(random.randrange(len(self.storage)))\n",
        "            self.storage.append((state, next_state, action, reward))\n",
        "\n",
        "    def sample(self, batch_size=100, flat=True):\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, next_states, actions, rewards= [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "            state, next_state, action, reward= self.storage[i]\n",
        "\n",
        "            if flat:\n",
        "                states.append(np.array(state, copy=False).flatten())\n",
        "                next_states.append(np.array(next_state, copy=False).flatten())\n",
        "            else:\n",
        "                states.append(np.array(state, copy=False))\n",
        "                next_states.append(np.array(next_state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(np.array(reward, copy=False))\n",
        "\n",
        "        # state_sample, action_sample, next_state_sample, reward_sample, done_sample\n",
        "        return {\n",
        "            \"state\": np.stack(states),\n",
        "            \"next_state\": np.stack(next_states),\n",
        "            \"action\": np.stack(actions),\n",
        "            \"reward\": np.stack(rewards).reshape(-1, 1),\n",
        "        }\n",
        "\n",
        "def evaluate_policy(env, policy, eval_episodes=10, max_timesteps=20):\n",
        "    from IPython import display\n",
        "    avg_reward = 0.0\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    for _ in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done and step < max_timesteps:\n",
        "            action = policy.predict(np.array(obs))\n",
        "            img.set_data(env.render(mode='rgb_array')) \n",
        "            plt.axis('off')\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            print(action, reward)\n",
        "            avg_reward += reward\n",
        "            step += 1\n",
        "    avg_reward /= eval_episodes\n",
        "    return avg_reward\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "from gym_duckietown.simulator import Simulator\n",
        "\n",
        "\n",
        "class MotionBlurWrapper(Simulator):\n",
        "    def __init__(self, env=None):\n",
        "        Simulator.__init__(self)\n",
        "        self.env = env\n",
        "        self.frame_skip = 3\n",
        "        self.env.delta_time = self.env.delta_time / self.frame_skip\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        action = np.clip(action, -1, 1)\n",
        "        # Actions could be a Python list\n",
        "        action = np.array(action)\n",
        "        motion_blur_window = []\n",
        "        for _ in range(self.frame_skip):\n",
        "            obs = self.env.render_obs()\n",
        "            motion_blur_window.append(obs)\n",
        "            self.env.update_physics(action)\n",
        "\n",
        "        # Generate the current camera image\n",
        "\n",
        "        obs = self.env.render_obs()\n",
        "        motion_blur_window.append(obs)\n",
        "        obs = np.average(motion_blur_window, axis=0, weights=[0.8, 0.15, 0.04, 0.01])\n",
        "\n",
        "        misc = self.env.get_agent_info()\n",
        "\n",
        "        d = self.env._compute_done_reward()\n",
        "        misc[\"Simulator\"][\"msg\"] = d.done_why\n",
        "\n",
        "        return obs, d.reward, d.done, misc\n",
        "\n",
        "\n",
        "class ResizeWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None, shape=(120, 160, 3)):\n",
        "        super(ResizeWrapper, self).__init__(env)\n",
        "        self.observation_space.shape = shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            self.observation_space.low[0, 0, 0],\n",
        "            self.observation_space.high[0, 0, 0],\n",
        "            shape,\n",
        "            dtype=self.observation_space.dtype,\n",
        "        )\n",
        "        self.shape = shape\n",
        "\n",
        "    def observation(self, observation):\n",
        "        #from PIL import image\n",
        "        #np.array(Image.fromarray(observation).resize(160, 120))\n",
        "        from scipy.misc import imresize\n",
        "\n",
        "        return imresize(observation, self.shape)\n",
        "\n",
        "\n",
        "class NormalizeWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(NormalizeWrapper, self).__init__(env)\n",
        "        self.obs_lo = self.observation_space.low[0, 0, 0]\n",
        "        self.obs_hi = self.observation_space.high[0, 0, 0]\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = spaces.Box(0.0, 1.0, obs_shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self.obs_lo == 0.0 and self.obs_hi == 1.0:\n",
        "            return obs\n",
        "        else:\n",
        "            return (obs - self.obs_lo) / (self.obs_hi - self.obs_lo)\n",
        "\n",
        "\n",
        "class ImgWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ImgWrapper, self).__init__(env)\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            self.observation_space.low[0, 0, 0],\n",
        "            self.observation_space.high[0, 0, 0],\n",
        "            [obs_shape[2], obs_shape[0], obs_shape[1]],\n",
        "            dtype=self.observation_space.dtype,\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return observation.transpose(2, 0, 1)\n",
        "\n",
        "\n",
        "class DtRewardWrapper(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(DtRewardWrapper, self).__init__(env)\n",
        "\n",
        "    def reward(self, rewardUnused):\n",
        "        try:\n",
        "            lane_pose = self.env.get_lane_pos2(self.env.cur_pos, self.env.cur_angle)\n",
        "            reward = 20.0 - (10 * abs(lane_pose.angle_rad) * abs(lane_pose.angle_rad)) - (300 * abs(lane_pose.dist) * abs(lane_pose.dist))\n",
        "        except:\n",
        "            reward = 0\n",
        "            self.env.reset()    \n",
        "        return reward\n",
        "\n",
        "\n",
        "# this is needed because at max speed the duckie can't turn anymore\n",
        "class ActionWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ActionWrapper, self).__init__(env)\n",
        "\n",
        "    def action(self, action):\n",
        "        action_ = [(0.5 + action[0] / 4), action[1]] # speed from +0.25 to +0.75\n",
        "        return action_        "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSr2AgZKNzg"
      },
      "source": [
        "import ast\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "\n",
        "# Duckietown Specific\n",
        "#from reinforcement.pytorch.ddpg import DDPG\n",
        "#from reinforcement.pytorch.utils import seed, evaluate_policy, ReplayBuffer\n",
        "#from utils.env import launch_env\n",
        "#from utils.wrappers import NormalizeWrapper, ImgWrapper, DtRewardWrapper, ActionWrapper, ResizeWrapper\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "def _train():\n",
        "\n",
        "\n",
        "    eval_freq = 5e3\n",
        "    max_timesteps = 10\n",
        "    save_models = False\n",
        "    expl_noise = 0.1\n",
        "    batch_size = 15\n",
        "    discount = 0.99\n",
        "    tau = 0.001\n",
        "    policy_noise = 0.2 \n",
        "    noise_clip = 0.5\n",
        "    policy_freq = 2\n",
        "    env_timesteps = 50\n",
        "    replay_buffer_max_size = 100\n",
        "    model_dir = \"\"\n",
        "    \n",
        "\n",
        "   # if not os.path.exists(\"./results\"):\n",
        "    #    os.makedirs(\"./results\")\n",
        "    #if not os.path.exists(args.model_dir):\n",
        "     #   os.makedirs(args.model_dir)\n",
        "\n",
        "    seed = 5\n",
        "    map_name = \"udem1\"\n",
        "    draw_curve = False\n",
        "    draw_bbox = False\n",
        "    domain_rand = False\n",
        "    frame_skip = 1\n",
        "    distortion = False\n",
        "    camera_rand = False\n",
        "    dynamics_rand = False\n",
        "\n",
        "    env = DuckietownEnv(\n",
        "        seed=seed,\n",
        "        map_name=map_name,\n",
        "        draw_curve=draw_curve,\n",
        "        draw_bbox=draw_bbox,\n",
        "        domain_rand=domain_rand,\n",
        "        frame_skip=frame_skip,\n",
        "        distortion=distortion,\n",
        "        camera_rand=camera_rand,\n",
        "        camera_width=160,\n",
        "        camera_height=120,\n",
        "        dynamics_rand=dynamics_rand,\n",
        "        max_steps=500000\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Initialized environment\")\n",
        "\n",
        "    # Wrappers\n",
        "    #env = ResizeWrapper(env)\n",
        "    #env = NormalizeWrapper(env)\n",
        "    env = ImgWrapper(env)  # to make the images from 160x120x3 into 3x160x120\n",
        "    env = ActionWrapper(env)\n",
        "    env = DtRewardWrapper(env)\n",
        "    print(\"Initialized Wrappers\")\n",
        "\n",
        "\n",
        "    state_dim = env.observation_space.shape\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize policy\n",
        "    policy = DDPG(state_dim, action_dim, max_action, net_type=\"cnn\")\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_max_size)\n",
        "    print(\"Initialized DDPG\")\n",
        "\n",
        "    # Evaluate untrained policy\n",
        "\n",
        "    #evaluations = []\n",
        "    #print(\"evaluations\")\n",
        "    total_timesteps = 0\n",
        "    #timesteps_since_eval = 0\n",
        "    #episode_num = 0\n",
        "    #done = True\n",
        "    #episode_reward = None\n",
        "    #env_counter = 0\n",
        "    reward = 0\n",
        "    #episode_timesteps = 0\n",
        "    T = 100\n",
        "\n",
        "\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    #initial replay buffer\n",
        "    while len(replay_buffer.storage) < replay_buffer_max_size:\n",
        "        obs = env.reset()\n",
        "        for i in range(int(replay_buffer_max_size / 10)):\n",
        "            action = policy.predict(np.array(obs))\n",
        "            img.set_data(env.render(mode='rgb_array')) \n",
        "            plt.axis('off')\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)  \n",
        "            print(\"step_count = %s, reward=%.3f\" % (env.unwrapped.step_count, reward))  \n",
        "            print(action)        \n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "            replay_buffer.add(obs, new_obs, action, reward)\n",
        "            obs = new_obs.copy()\n",
        "\n",
        "\n",
        "    #max timesteps M\n",
        "    for total_timesteps in range(max_timesteps):\n",
        "        print(f\"Epoch {total_timesteps}\")\n",
        "        #recieve initial observation state\n",
        "        obs = env.reset()\n",
        "        for it in range(T):\n",
        "            #select action\n",
        "            action = policy.predict(np.array(obs))\n",
        "            print('action', action)\n",
        "            img.set_data(env.render(mode='rgb_array')) \n",
        "            plt.axis('off')\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True) \n",
        "            #if expl_noise != 0:\n",
        "            #    action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(\n",
        "            #        env.action_space.low, env.action_space.high\n",
        "            #    )\n",
        "            #print('noize action', action)    \n",
        "            #execute action\n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "            #store transition\n",
        "            replay_buffer.add(obs, new_obs, action, reward)\n",
        "\n",
        "            # Sample random minibatch from replay buffer\n",
        "            sample = replay_buffer.sample(batch_size, flat=policy.flat)\n",
        "            state = torch.FloatTensor(sample[\"state\"]).to(device)\n",
        "            action = torch.FloatTensor(sample[\"action\"]).to(device)\n",
        "            next_state = torch.FloatTensor(sample[\"next_state\"]).to(device)\n",
        "            reward = torch.FloatTensor(sample[\"reward\"]).to(device)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q = policy.critic_target(next_state, policy.actor_target(next_state))\n",
        "            target_Q = reward + (discount * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimate\n",
        "            current_Q = policy.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "            # Update the critic\n",
        "            policy.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            policy.critic_optimizer.step()\n",
        "\n",
        "            # Compute actor loss\n",
        "            actor_loss = -policy.critic(state, policy.actor(state)).mean()\n",
        "\n",
        "            # Update the actor\n",
        "            policy.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            policy.actor_optimizer.step()\n",
        "\n",
        "            # Update the target networks\n",
        "            for param, target_param in zip(policy.critic.parameters(), policy.critic_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(policy.actor.parameters(), policy.actor_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "            obs = new_obs.copy()\n",
        "        \n",
        "        #evaluate policy    \n",
        "        import time\n",
        "        print(\"Evaluation\")\n",
        "        time.sleep(1)    \n",
        "        #evaluate_policy(env, policy)  \n",
        "        #time.sleep(1)   \n",
        "\n",
        "        #save checkpoint   \n",
        "        if save_models:\n",
        "            policy.save(file_name=\"ddpg\", directory=model_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    _train()    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5iVEHY2KW7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "01fd065e-df59-49cb-cadf-6491f94954e6"
      },
      "source": [
        "import ast\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Duckietown Specific\n",
        "from reinforcement.pytorch.ddpg import DDPG\n",
        "from utils.env import launch_env\n",
        "from utils.wrappers import NormalizeWrapper, ImgWrapper, DtRewardWrapper, ActionWrapper, ResizeWrapper\n",
        "\n",
        "\n",
        "def _enjoy():\n",
        "    # Launch the env with our helper function\n",
        "    env = launch_env()\n",
        "    print(\"Initialized environment\")\n",
        "\n",
        "    # Wrappers\n",
        "    env = ResizeWrapper(env)\n",
        "    env = NormalizeWrapper(env)\n",
        "    env = ImgWrapper(env)  # to make the images from 160x120x3 into 3x160x120\n",
        "    env = ActionWrapper(env)\n",
        "    env = DtRewardWrapper(env)\n",
        "    print(\"Initialized Wrappers\")\n",
        "\n",
        "    state_dim = env.observation_space.shape\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize policy\n",
        "    policy = DDPG(state_dim, action_dim, max_action, net_type=\"cnn\")\n",
        "    policy.load(filename=\"ddpg\", directory=\"reinforcement/pytorch/models/\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        while not done:\n",
        "            action = policy.predict(np.array(obs))\n",
        "            # Perform action\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            env.render()\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    _enjoy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-493a50c59142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Duckietown Specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreinforcement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddpg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlaunch_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizeWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImgWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDtRewardWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActionWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResizeWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reinforcement'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}